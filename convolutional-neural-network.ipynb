{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":101849,"databundleVersionId":13093295,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# First Model: CNN","metadata":{}},{"cell_type":"markdown","source":"## Import packages","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport itertools\nimport os\nimport glob \nimport seaborn as sns\nimport tensorflow as tf\nimport multiprocessing as mp\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Lambda\nfrom tensorflow.keras.models import Model\nfrom astropy.stats import sigma_clip\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nsns.set_theme(style='dark')\npalette = sns.color_palette('muted')\npd.set_option('display.max_columns', None)\n\nprint('TF version:', tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-28T01:07:13.052861Z","iopub.execute_input":"2025-09-28T01:07:13.053076Z","iopub.status.idle":"2025-09-28T01:07:37.334016Z","shell.execute_reply.started":"2025-09-28T01:07:13.053059Z","shell.execute_reply":"2025-09-28T01:07:37.333411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load and calibrate the data","metadata":{}},{"cell_type":"code","source":"path_folder = '/kaggle/input/ariel-data-challenge-2025/' \npath_out = '/kaggle/tmp/data_light_raw/'\n\nif not os.path.exists(path_out):\n    os.makedirs(path_out)\n    print(f\"Directory {path_out} created.\")\nelse:\n    print(f\"Directory {path_out} already exists.\")\n\nCHUNKS_SIZE = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T01:07:37.334721Z","iopub.execute_input":"2025-09-28T01:07:37.335258Z","iopub.status.idle":"2025-09-28T01:07:37.341462Z","shell.execute_reply.started":"2025-09-28T01:07:37.335230Z","shell.execute_reply":"2025-09-28T01:07:37.340027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ADC_convert(signal, gain=0.4369, offset=-1000):\n    \"\"\"The Analog-to-Digital Conversion (adc) is performed by the detector to convert\n    the pixel voltage into an integer number. Since we are using the same conversion number \n    this year, we have simply hard-coded it inside.\"\"\"\n    signal = signal.astype(np.float64)\n    signal /= gain\n    signal += offset\n    return signal\n\ndef mask_hot_dead(signal, dead, dark):\n    hot = sigma_clip(\n        dark, sigma=5, maxiters=5\n    ).mask\n    hot = np.tile(hot, (signal.shape[0], 1, 1))\n    dead = np.tile(dead, (signal.shape[0], 1, 1))\n    signal = np.ma.masked_where(dead, signal)\n    signal = np.ma.masked_where(hot, signal)\n    return signal\n\ndef apply_linear_corr(linear_corr, clean_signal):\n    linear_corr = np.flip(linear_corr, axis=0)\n    for x, y in itertools.product(\n                range(clean_signal.shape[1]), range(clean_signal.shape[2])\n            ):\n        poli = np.poly1d(linear_corr[:, x, y])\n        clean_signal[:, x, y] = poli(clean_signal[:, x, y])\n    return clean_signal\n\ndef clean_dark(signal, dead, dark, dt):\n    dark = np.ma.masked_where(dead, dark)\n    dark = np.tile(dark, (signal.shape[0], 1, 1))\n    signal -= dark * dt[:, np.newaxis, np.newaxis]\n    return signal\n\ndef get_cds(signal):\n    cds = signal[:,1::2,:,:] - signal[:,::2,:,:]\n    return cds\n\ndef bin_obs(cds_signal, binning):\n    cds_transposed = cds_signal.transpose(0,1,3,2)\n    cds_binned = np.zeros((cds_transposed.shape[0], cds_transposed.shape[1] // binning, cds_transposed.shape[2], cds_transposed.shape[3]))\n    for i in range(cds_transposed.shape[1] // binning):\n        cds_binned[:,i,:,:] = np.sum(cds_transposed[:,i*binning:(i+1)*binning,:,:], axis=1)\n    return cds_binned\n\ndef correct_flat_field(flat, dead, signal):\n    flat = flat.transpose(1, 0)\n    dead = dead.transpose(1, 0)\n    flat = np.ma.masked_where(dead, flat)\n    flat = np.tile(flat, (signal.shape[0], 1, 1))\n    signal /= flat\n    return signal\n\ndef get_index(files, CHUNKS_SIZE):\n    index = []\n    for file in files:\n        file_name = file.split('/')[-1]\n        if file_name.split('_')[0] == 'AIRS-CH0' and file_name.split('_')[1] == 'signal' and file_name.split('_')[2] == '0.parquet':\n            file_index = os.path.basename(os.path.dirname(file))\n            index.append(int(file_index))\n    index = np.array(index)\n    index = np.sort(index) \n    # credit to DennisSakva\n    index = np.array_split(index, len(index) // CHUNKS_SIZE)\n    \n    return index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T01:07:37.344125Z","iopub.execute_input":"2025-09-28T01:07:37.344883Z","iopub.status.idle":"2025-09-28T01:07:37.382072Z","shell.execute_reply.started":"2025-09-28T01:07:37.344852Z","shell.execute_reply":"2025-09-28T01:07:37.381407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"files = glob.glob(os.path.join(path_folder + 'train/', '*/*'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T01:07:37.382827Z","iopub.execute_input":"2025-09-28T01:07:37.383052Z","iopub.status.idle":"2025-09-28T01:07:42.437310Z","shell.execute_reply.started":"2025-09-28T01:07:37.383028Z","shell.execute_reply":"2025-09-28T01:07:42.436771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index = get_index(files, CHUNKS_SIZE)\nprint(len(index[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T01:07:42.437997Z","iopub.execute_input":"2025-09-28T01:07:42.438196Z","iopub.status.idle":"2025-09-28T01:07:42.447360Z","shell.execute_reply.started":"2025-09-28T01:07:42.438179Z","shell.execute_reply":"2025-09-28T01:07:42.446628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"axis_info = pd.read_parquet(os.path.join(path_folder,'axis_info.parquet'))\nDO_MASK = False\nDO_THE_NL_CORR = False\nDO_DARK = False\nDO_FLAT = False\nTIME_BINNING = True\n\ncut_inf, cut_sup = 39, 321\nl = cut_sup - cut_inf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T01:07:42.448241Z","iopub.execute_input":"2025-09-28T01:07:42.448512Z","iopub.status.idle":"2025-09-28T01:07:42.715576Z","shell.execute_reply.started":"2025-09-28T01:07:42.448486Z","shell.execute_reply":"2025-09-28T01:07:42.715029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_calibration_data_batch(path_folder, index_chunk, cut_inf, cut_sup, dataset):\n    \"\"\"Load all calibration data for the chunk at once\"\"\"\n    calibration_data = {}\n    \n    for idx in index_chunk:\n        calibration_data[idx] = {}\n        \n        # AIRS calibration data\n        airs_flat = pd.read_parquet(os.path.join(path_folder, f'{dataset}/{idx}/AIRS-CH0_calibration_0/flat.parquet'))\n        calibration_data[idx]['airs_flat'] = airs_flat.values.astype(np.float32).reshape((32, 356))[:, cut_inf:cut_sup]\n        \n        airs_dark = pd.read_parquet(os.path.join(path_folder, f'{dataset}/{idx}/AIRS-CH0_calibration_0/dark.parquet'))\n        calibration_data[idx]['airs_dark'] = airs_dark.values.astype(np.float32).reshape((32, 356))[:, cut_inf:cut_sup]\n        \n        airs_dead = pd.read_parquet(os.path.join(path_folder, f'{dataset}/{idx}/AIRS-CH0_calibration_0/dead.parquet'))\n        calibration_data[idx]['airs_dead'] = airs_dead.values.astype(np.float32).reshape((32, 356))[:, cut_inf:cut_sup]\n        \n        airs_linear = pd.read_parquet(os.path.join(path_folder, f'{dataset}/{idx}/AIRS-CH0_calibration_0/linear_corr.parquet'))\n        calibration_data[idx]['airs_linear'] = airs_linear.values.astype(np.float32).reshape((6, 32, 356))[:, :, cut_inf:cut_sup]\n        \n        # FGS1 calibration data\n        fgs_flat = pd.read_parquet(os.path.join(path_folder, f'{dataset}/{idx}/FGS1_calibration_0/flat.parquet'))\n        calibration_data[idx]['fgs_flat'] = fgs_flat.values.astype(np.float32).reshape((32, 32))\n        \n        fgs_dark = pd.read_parquet(os.path.join(path_folder, f'{dataset}/{idx}/FGS1_calibration_0/dark.parquet'))\n        calibration_data[idx]['fgs_dark'] = fgs_dark.values.astype(np.float32).reshape((32, 32))\n        \n        fgs_dead = pd.read_parquet(os.path.join(path_folder, f'{dataset}/{idx}/FGS1_calibration_0/dead.parquet'))\n        calibration_data[idx]['fgs_dead'] = fgs_dead.values.astype(np.float32).reshape((32, 32))\n        \n        fgs_linear = pd.read_parquet(os.path.join(path_folder, f'{dataset}/{idx}/FGS1_calibration_0/linear_corr.parquet'))\n        calibration_data[idx]['fgs_linear'] = fgs_linear.values.astype(np.float32).reshape((6, 32, 32))\n    \n    return calibration_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T01:07:42.716334Z","iopub.execute_input":"2025-09-28T01:07:42.716601Z","iopub.status.idle":"2025-09-28T01:07:42.724923Z","shell.execute_reply.started":"2025-09-28T01:07:42.716580Z","shell.execute_reply":"2025-09-28T01:07:42.724224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_single_observation(args):\n    \"\"\"Process a single observation with all AIRS and FGS1 cleaning steps\"\"\"\n    # Unpack the arguments\n    (i, index_chunk, path_folder, cut_inf, cut_sup, l, axis_info, calibration_data, DO_MASK, DO_THE_NL_CORR, DO_DARK) = args\n    \n    idx = index_chunk[i]\n    \n    # AIRS Processing\n    # Load AIRS signal data\n    df = pd.read_parquet(os.path.join(path_folder, f'train/{idx}/AIRS-CH0_signal_0.parquet'))\n    signal = df.values.astype(np.float32).reshape((df.shape[0], 32, 356))\n\n    # 1. ADC Conversion\n    signal = ADC_convert(signal)\n    dt_airs = axis_info['AIRS-CH0-integration_time'].dropna().values\n    dt_airs[1::2] += 0.1\n    chopped_signal = signal[:, :, cut_inf:cut_sup]\n    del signal, df\n    \n    # Get pre-loaded calibration data for AIRS\n    flat = calibration_data[idx]['airs_flat']\n    dark = calibration_data[idx]['airs_dark'] \n    dead_airs = calibration_data[idx]['airs_dead']\n    linear_corr = calibration_data[idx]['airs_linear']\n\n    # 2. Mask Hot/Dead Pixels\n    if DO_MASK:\n        chopped_signal = mask_hot_dead(chopped_signal, dead_airs, dark)\n\n    # 3. Linearity Correction\n    if DO_THE_NL_CORR: \n        linear_corr_signal = apply_linear_corr(linear_corr, chopped_signal)\n        chopped_signal = linear_corr_signal\n\n    # 4. Dark Current Subtraction\n    if DO_DARK: \n        cleaned_signal = clean_dark(chopped_signal, dead_airs, dark, dt_airs)\n        chopped_signal = cleaned_signal\n\n    # Store AIRS result\n    airs_result = chopped_signal\n    \n    # FGS1 Processing\n    # Load FGS1 signal data\n    df = pd.read_parquet(os.path.join(path_folder, f'train/{idx}/FGS1_signal_0.parquet'))\n    fgs_signal = df.values.astype(np.float32).reshape((df.shape[0], 32, 32))\n\n    # 1. ADC Conversion\n    fgs_signal = ADC_convert(fgs_signal)\n    dt_fgs1 = np.ones(len(fgs_signal)) * 0.1\n    dt_fgs1[1::2] += 0.1\n    chopped_FGS1 = fgs_signal\n    del fgs_signal, df\n    \n    # Get pre-loaded calibration data for FGS1\n    flat = calibration_data[idx]['fgs_flat']\n    dark = calibration_data[idx]['fgs_dark']\n    dead_fgs1 = calibration_data[idx]['fgs_dead']\n    linear_corr = calibration_data[idx]['fgs_linear']\n\n    # 2. Mask Hot/Dead pixels\n    if DO_MASK:\n        chopped_FGS1 = mask_hot_dead(chopped_FGS1, dead_fgs1, dark)\n\n    # 3. Linearity Correction\n    if DO_THE_NL_CORR: \n        linear_corr_signal = apply_linear_corr(linear_corr, chopped_FGS1)\n        chopped_FGS1 = linear_corr_signal\n\n    # 4. Dark Current Subtraction\n    if DO_DARK: \n        cleaned_signal = clean_dark(chopped_FGS1, dead_fgs1, dark, dt_fgs1)\n        chopped_FGS1 = cleaned_signal\n\n    # Store FGS1 result\n    fgs_result = chopped_FGS1\n    \n    # Return the processed results\n    return i, airs_result, fgs_result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T01:07:42.725729Z","iopub.execute_input":"2025-09-28T01:07:42.725965Z","iopub.status.idle":"2025-09-28T01:07:42.747551Z","shell.execute_reply.started":"2025-09-28T01:07:42.725941Z","shell.execute_reply":"2025-09-28T01:07:42.747076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_single_observation_test(args):\n    \"\"\"Process a single observation with all AIRS and FGS1 cleaning steps\"\"\"\n    # Unpack the arguments\n    (i, index_chunk, path_folder, cut_inf, cut_sup, l, axis_info, calibration_data, DO_MASK, DO_THE_NL_CORR, DO_DARK) = args\n    \n    idx = index_chunk[i]\n    \n    # AIRS Processing\n    # Load AIRS signal data\n    df = pd.read_parquet(os.path.join(path_folder, f'test/{idx}/AIRS-CH0_signal_0.parquet'))\n    signal = df.values.astype(np.float32).reshape((df.shape[0], 32, 356))\n\n    # 1. ADC Conversion\n    signal = ADC_convert(signal)\n    dt_airs = axis_info['AIRS-CH0-integration_time'].dropna().values\n    dt_airs[1::2] += 0.1\n    chopped_signal = signal[:, :, cut_inf:cut_sup]\n    del signal, df\n    \n    # Get pre-loaded calibration data for AIRS\n    flat = calibration_data[idx]['airs_flat']\n    dark = calibration_data[idx]['airs_dark'] \n    dead_airs = calibration_data[idx]['airs_dead']\n    linear_corr = calibration_data[idx]['airs_linear']\n\n    # 2. Mask Hot/Dead Pixels\n    if DO_MASK:\n        chopped_signal = mask_hot_dead(chopped_signal, dead_airs, dark)\n\n    # 3. Linearity Correction\n    if DO_THE_NL_CORR: \n        linear_corr_signal = apply_linear_corr(linear_corr, chopped_signal)\n        chopped_signal = linear_corr_signal\n\n    # 4. Dark Current Subtraction\n    if DO_DARK: \n        cleaned_signal = clean_dark(chopped_signal, dead_airs, dark, dt_airs)\n        chopped_signal = cleaned_signal\n\n    # Store AIRS result\n    airs_result = chopped_signal\n    \n    # FGS1 Processing\n    # Load FGS1 signal data\n    df = pd.read_parquet(os.path.join(path_folder, f'test/{idx}/FGS1_signal_0.parquet'))\n    fgs_signal = df.values.astype(np.float32).reshape((df.shape[0], 32, 32))\n\n    # 1. ADC Conversion\n    fgs_signal = ADC_convert(fgs_signal)\n    dt_fgs1 = np.ones(len(fgs_signal)) * 0.1\n    dt_fgs1[1::2] += 0.1\n    chopped_FGS1 = fgs_signal\n    del fgs_signal, df\n    \n    # Get pre-loaded calibration data for FGS1\n    flat = calibration_data[idx]['fgs_flat']\n    dark = calibration_data[idx]['fgs_dark']\n    dead_fgs1 = calibration_data[idx]['fgs_dead']\n    linear_corr = calibration_data[idx]['fgs_linear']\n\n    # 2. Mask Hot/Dead pixels\n    if DO_MASK:\n        chopped_FGS1 = mask_hot_dead(chopped_FGS1, dead_fgs1, dark)\n\n    # 3. Linearity Correction\n    if DO_THE_NL_CORR: \n        linear_corr_signal = apply_linear_corr(linear_corr, chopped_FGS1)\n        chopped_FGS1 = linear_corr_signal\n\n    # 4. Dark Current Subtraction\n    if DO_DARK: \n        cleaned_signal = clean_dark(chopped_FGS1, dead_fgs1, dark, dt_fgs1)\n        chopped_FGS1 = cleaned_signal\n\n    # Store FGS1 result\n    fgs_result = chopped_FGS1\n    \n    # Return the processed results\n    return i, airs_result, fgs_result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T01:07:42.749805Z","iopub.execute_input":"2025-09-28T01:07:42.750002Z","iopub.status.idle":"2025-09-28T01:07:42.769672Z","shell.execute_reply.started":"2025-09-28T01:07:42.749987Z","shell.execute_reply":"2025-09-28T01:07:42.769179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for n, index_chunk in enumerate(tqdm(index)):\n    # Load all calibration data once at the beginning \n    calibration_data = load_calibration_data_batch(path_folder, index_chunk, cut_inf, cut_sup, 'train')\n    \n    # Pre-allocate output arrays\n    AIRS_CH0_clean = np.zeros((CHUNKS_SIZE, 11250, 32, l), dtype=np.float32)\n    FGS1_clean = np.zeros((CHUNKS_SIZE, 135000, 32, 32), dtype=np.float32)\n    \n    # Parallel Processing\n    # Determine number of workers \n    num_workers = min(2, CHUNKS_SIZE)\n    \n    # Prepare arguments for each observation\n    args_list = []\n    for i in range(CHUNKS_SIZE):\n        args = (i, index_chunk, path_folder, cut_inf, cut_sup, l, axis_info, calibration_data, DO_MASK, DO_THE_NL_CORR, DO_DARK)\n        args_list.append(args)\n        \n    # Process observations in parallel\n    results = []\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:        \n        # Submit all tasks to the thread pool\n        future_to_index = {executor.submit(process_single_observation, args): args[0] for args in args_list}\n                \n        # Collect results as they complete\n        for future in tqdm(as_completed(future_to_index), total=CHUNKS_SIZE, desc=f\"Processing observations\"):\n            i, airs_result, fgs_result = future.result()\n            results.append((i, airs_result, fgs_result))\n        \n    # Sort results by observation index (i) to maintain order\n    results.sort(key=lambda x: x[0])\n    \n    # Store results in your existing arrays\n    for result in results:\n        i, airs_result, fgs_result = result\n        AIRS_CH0_clean[i] = airs_result\n        FGS1_clean[i] = fgs_result\n        \n    # 5. Get Correlated Double Sampling\n    AIRS_cds = get_cds(AIRS_CH0_clean)\n    FGS1_cds = get_cds(FGS1_clean)\n    \n    del AIRS_CH0_clean, FGS1_clean\n    \n    # 6. (Optional) Time Binning (to reduce space)\n    if TIME_BINNING:\n        AIRS_cds_binned = bin_obs(AIRS_cds, binning=30)\n        FGS1_cds_binned = bin_obs(FGS1_cds, binning=30*12)\n    else:\n        AIRS_cds = AIRS_cds.transpose(0,1,3,2)\n        AIRS_cds_binned = AIRS_cds\n        FGS1_cds = FGS1_cds.transpose(0,1,3,2)\n        FGS1_cds_binned = FGS1_cds\n    \n    del AIRS_cds, FGS1_cds\n\n    # 7. Flat Field Correction - use pre-loaded calibration data\n    for i in range(CHUNKS_SIZE):\n        if DO_FLAT:\n            flat_airs = calibration_data[index_chunk[i]]['airs_flat']  # Fixed: added [i]\n            flat_fgs = calibration_data[index_chunk[i]]['fgs_flat']    # Fixed: added [i]\n            dead_airs = calibration_data[index_chunk[i]]['airs_dead']  # Fixed: added [i]\n            dead_fgs1 = calibration_data[index_chunk[i]]['fgs_dead']   # Fixed: added [i]\n            \n            corrected_AIRS_cds_binned = correct_flat_field(flat_airs, dead_airs, AIRS_cds_binned[i])\n            AIRS_cds_binned[i] = corrected_AIRS_cds_binned\n            corrected_FGS1_cds_binned = correct_flat_field(flat_fgs, dead_fgs1, FGS1_cds_binned[i])\n            FGS1_cds_binned[i] = corrected_FGS1_cds_binned\n\n    # Save data\n    np.save(os.path.join(path_out, 'AIRS_clean_train_{}.npy'.format(n)), AIRS_cds_binned)\n    np.save(os.path.join(path_out, 'FGS1_train_{}.npy'.format(n)), FGS1_cds_binned)\n    del AIRS_cds_binned, FGS1_cds_binned, calibration_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T01:07:42.770457Z","iopub.execute_input":"2025-09-28T01:07:42.770681Z","iopub.status.idle":"2025-09-28T02:33:50.139416Z","shell.execute_reply.started":"2025-09-28T01:07:42.770657Z","shell.execute_reply":"2025-09-28T02:33:50.138638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_CHUNKS_SIZE = CHUNKS_SIZE\nCHUNKS_SIZE = 1\ntest_files = glob.glob(os.path.join(path_folder + 'test/', '*/*'))\ntest_index = get_index(test_files, CHUNKS_SIZE)\n\nfor n, index_chunk in enumerate(tqdm(test_index)):\n    # Load all calibration data once at the beginning \n    calibration_data = load_calibration_data_batch(path_folder, index_chunk, cut_inf, cut_sup, 'test')\n    \n    # Pre-allocate output arrays\n    AIRS_CH0_clean = np.zeros((CHUNKS_SIZE, 11250, 32, l), dtype=np.float32)\n    FGS1_clean = np.zeros((CHUNKS_SIZE, 135000, 32, 32), dtype=np.float32)\n    \n    # Parallel Processing\n    # Determine number of workers (start with 2 to be safe)\n    num_workers = min(2, CHUNKS_SIZE)\n    \n    # Prepare arguments for each observation\n    args_list = []\n    for i in range(CHUNKS_SIZE):\n        args = (i, index_chunk, path_folder, cut_inf, cut_sup, l, axis_info, calibration_data, DO_MASK, DO_THE_NL_CORR, DO_DARK)\n        args_list.append(args)\n        \n    # Process observations in parallel\n    results = []\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:        \n        # Submit all tasks to the thread pool\n        future_to_index = {executor.submit(process_single_observation_test, args): args[0] for args in args_list}\n                \n        # Collect results as they complete\n        for future in tqdm(as_completed(future_to_index), total=CHUNKS_SIZE, desc=f\"Processing observations\"):\n            i, airs_result, fgs_result = future.result()\n            results.append((i, airs_result, fgs_result))\n        \n    # Sort results by observation index (i) to maintain order\n    results.sort(key=lambda x: x[0])\n    \n    # Store results in your existing arrays\n    for result in results:\n        i, airs_result, fgs_result = result\n        AIRS_CH0_clean[i] = airs_result\n        FGS1_clean[i] = fgs_result\n        \n    # 5. Get Correlated Double Sampling\n    AIRS_cds = get_cds(AIRS_CH0_clean)\n    FGS1_cds = get_cds(FGS1_clean)\n    \n    del AIRS_CH0_clean, FGS1_clean\n    \n    # 6. (Optional) Time Binning (to reduce space)\n    if TIME_BINNING:\n        AIRS_cds_binned = bin_obs(AIRS_cds, binning=30)\n        FGS1_cds_binned = bin_obs(FGS1_cds, binning=30*12)\n    else:\n        AIRS_cds = AIRS_cds.transpose(0,1,3,2)\n        AIRS_cds_binned = AIRS_cds\n        FGS1_cds = FGS1_cds.transpose(0,1,3,2)\n        FGS1_cds_binned = FGS1_cds\n    \n    del AIRS_cds, FGS1_cds\n\n    # 7. Flat Field Correction - use pre-loaded calibration data\n    for i in range(CHUNKS_SIZE):\n        if DO_FLAT:\n            flat_airs = calibration_data[index_chunk[i]]['airs_flat']  # Fixed: added [i]\n            flat_fgs = calibration_data[index_chunk[i]]['fgs_flat']    # Fixed: added [i]\n            dead_airs = calibration_data[index_chunk[i]]['airs_dead']  # Fixed: added [i]\n            dead_fgs1 = calibration_data[index_chunk[i]]['fgs_dead']   # Fixed: added [i]\n            \n            corrected_AIRS_cds_binned = correct_flat_field(flat_airs, dead_airs, AIRS_cds_binned[i])\n            AIRS_cds_binned[i] = corrected_AIRS_cds_binned\n            corrected_FGS1_cds_binned = correct_flat_field(flat_fgs, dead_fgs1, FGS1_cds_binned[i])\n            FGS1_cds_binned[i] = corrected_FGS1_cds_binned\n\n    # Save data\n    np.save(os.path.join(path_out, 'AIRS_clean_test_{}.npy'.format(n)), AIRS_cds_binned)\n    np.save(os.path.join(path_out, 'FGS1_test_{}.npy'.format(n)), FGS1_cds_binned)\n    del AIRS_cds_binned, FGS1_cds_binned, calibration_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T02:33:50.140401Z","iopub.execute_input":"2025-09-28T02:33:50.140949Z","iopub.status.idle":"2025-09-28T02:33:58.541572Z","shell.execute_reply.started":"2025-09-28T02:33:50.140923Z","shell.execute_reply":"2025-09-28T02:33:58.540933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(file, chunk_size, nb_files): \n    data0 = np.load(file + '_0.npy')\n    data_all = np.zeros((nb_files * chunk_size, data0.shape[1], data0.shape[2], data0.shape[3]))\n    data_all[:chunk_size] = data0\n    for i in range(1, nb_files): \n        data_all[i * chunk_size : (i+1) * chunk_size] = np.load(file + '_{}.npy'.format(i))\n    return data_all \n\ndata_train_AIRS = load_data(path_out + 'AIRS_clean_train', TRAIN_CHUNKS_SIZE, len(index)) \ndata_train_FGS = load_data(path_out + 'FGS1_train', TRAIN_CHUNKS_SIZE, len(index))\n\nprint(data_train_AIRS.shape)\nprint(data_train_FGS.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T02:33:58.542239Z","iopub.execute_input":"2025-09-28T02:33:58.542533Z","iopub.status.idle":"2025-09-28T02:35:35.414783Z","shell.execute_reply.started":"2025-09-28T02:33:58.542506Z","shell.execute_reply":"2025-09-28T02:35:35.414118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_test_AIRS = load_data(path_out + 'AIRS_clean_test', CHUNKS_SIZE, 1) \ndata_test_FGS = load_data(path_out + 'FGS1_test', CHUNKS_SIZE, 1)\n\nprint(data_test_AIRS.shape)\nprint(data_test_FGS.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T02:35:35.415515Z","iopub.execute_input":"2025-09-28T02:35:35.415699Z","iopub.status.idle":"2025-09-28T02:35:35.497164Z","shell.execute_reply.started":"2025-09-28T02:35:35.415684Z","shell.execute_reply":"2025-09-28T02:35:35.496612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(path_folder + 'train.csv')\ndf_train.set_index('planet_id', inplace=True)\n\nplanet_ids = np.concatenate(index)\ndf_train = df_train[df_train.index.isin(planet_ids)]\n\nprint(df_train.shape)\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T02:35:35.497798Z","iopub.execute_input":"2025-09-28T02:35:35.498022Z","iopub.status.idle":"2025-09-28T02:35:35.868619Z","shell.execute_reply.started":"2025-09-28T02:35:35.498005Z","shell.execute_reply":"2025-09-28T02:35:35.867842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split into train and validation sets","metadata":{}},{"cell_type":"code","source":"n = round(.8 * len(df_train))\nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T02:35:35.869360Z","iopub.execute_input":"2025-09-28T02:35:35.869863Z","iopub.status.idle":"2025-09-28T02:35:35.874873Z","shell.execute_reply.started":"2025-09-28T02:35:35.869836Z","shell.execute_reply":"2025-09-28T02:35:35.874219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_AIRS = data_train_AIRS[:n]\nval_AIRS = data_train_AIRS[n:]\nprint(len(train_AIRS), len(val_AIRS))\n\ntrain_FGS = data_train_FGS[:n]\nval_FGS = data_train_FGS[n:]\nprint(len(train_FGS), len(val_FGS))\n\ntrain_labels = df_train.iloc[:n,:] \nval_labels = df_train.iloc[n:,:]\nprint(train_labels.shape, val_labels.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T02:35:35.875573Z","iopub.execute_input":"2025-09-28T02:35:35.875778Z","iopub.status.idle":"2025-09-28T02:35:35.892523Z","shell.execute_reply.started":"2025-09-28T02:35:35.875763Z","shell.execute_reply":"2025-09-28T02:35:35.891882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define the model","metadata":{}},{"cell_type":"code","source":"print(train_AIRS.shape)\nprint(train_labels.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T02:35:35.893236Z","iopub.execute_input":"2025-09-28T02:35:35.893570Z","iopub.status.idle":"2025-09-28T02:35:35.908178Z","shell.execute_reply.started":"2025-09-28T02:35:35.893546Z","shell.execute_reply":"2025-09-28T02:35:35.907573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = Input(shape=(375, 282, 32), name='inputs')\nx = Conv2D(32, (3, 3), activation='relu')(inputs)\nx = MaxPooling2D((2, 2))(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = MaxPooling2D((2, 2))(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = Flatten()(x)\nx = Dense(64, activation='relu')(x)\n\n# Two output heads\nmean_output = Dense(283, activation='linear', name='mean')(x)  \nlog_std_output = Dense(283, activation='linear', name='log_std')(x) \nstd_output = Lambda(lambda x: tf.exp(0.5 * x), name='std')(log_std_output)\n\n# Concatenate outputs for submission\noutputs = Concatenate(name='outputs')([mean_output, std_output])\n\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T02:35:35.908869Z","iopub.execute_input":"2025-09-28T02:35:35.909064Z","iopub.status.idle":"2025-09-28T02:35:40.763533Z","shell.execute_reply.started":"2025-09-28T02:35:35.909042Z","shell.execute_reply":"2025-09-28T02:35:40.762814Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compile and train the model","metadata":{}},{"cell_type":"code","source":"def nll_loss(y_true, y_pred):\n    mu, std = y_pred[:, :283], y_pred[:, 283:]\n    return tf.reduce_mean(0.5 * tf.math.log(2 * np.pi * std**2) + 0.5 * ((y_true - mu)**2 / std**2))\n\nmodel.compile(optimizer = 'adam', loss = nll_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T02:35:40.764224Z","iopub.execute_input":"2025-09-28T02:35:40.764510Z","iopub.status.idle":"2025-09-28T02:35:40.777901Z","shell.execute_reply.started":"2025-09-28T02:35:40.764486Z","shell.execute_reply":"2025-09-28T02:35:40.777288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(train_AIRS, train_labels.values, \n          validation_data=(val_AIRS, val_labels.values),\n          epochs=10, batch_size=32, verbose=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate predictions","metadata":{}},{"cell_type":"code","source":"# Generate predictions\npredictions = model.predict(data_test_AIRS)  \nmeans = predictions[:, :283]  \nstds = predictions[:, 283:] \nstds = np.abs(stds)\n\n# Create submission DataFrame\n# Fix test_index if it's nested from chunking\nif isinstance(test_index, list) and len(test_index) > 0:\n    test_index_flat = np.concatenate(test_index)\nelse:\n    test_index_flat = test_index\n    \n# Load sample submission \ndf_sample = pd.read_csv(path_folder + 'sample_submission.csv')\n\n# Create submission DataFrame matching sample format exactly\ndf_submission = df_sample.copy()\ndf_submission['planet_id'] = test_index_flat\n\ndf_submission.iloc[:, 1:284] = means.astype(np.float32)  \ndf_submission.iloc[:, 284:567] = stds.astype(np.float32)   \n\n# Replace inf and NaN values\ndf_submission = df_submission.replace([np.inf, -np.inf], np.nan)\ndf_submission = df_submission.fillna(0.0)\n\n# Ensure all columns have correct data types (skip planet_id)\nfor col in df_submission.columns[1:]: \n    df_submission[col] = df_submission[col].astype(np.float32)\n\n# Verify\nprint(df_submission.shape) \ndf_submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T03:16:07.336318Z","iopub.execute_input":"2025-09-28T03:16:07.336633Z","iopub.status.idle":"2025-09-28T03:16:07.673507Z","shell.execute_reply.started":"2025-09-28T03:16:07.336612Z","shell.execute_reply":"2025-09-28T03:16:07.672814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T03:16:14.163704Z","iopub.execute_input":"2025-09-28T03:16:14.164341Z","iopub.status.idle":"2025-09-28T03:16:14.171276Z","shell.execute_reply.started":"2025-09-28T03:16:14.164316Z","shell.execute_reply":"2025-09-28T03:16:14.170529Z"}},"outputs":[],"execution_count":null}]}